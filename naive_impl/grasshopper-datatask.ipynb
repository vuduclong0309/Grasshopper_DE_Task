{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-30T10:39:58.058207Z","iopub.execute_input":"2022-08-30T10:39:58.058682Z","iopub.status.idle":"2022-08-30T10:39:58.070696Z","shell.execute_reply.started":"2022-08-30T10:39:58.058627Z","shell.execute_reply":"2022-08-30T10:39:58.068777Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:39:58.073600Z","iopub.execute_input":"2022-08-30T10:39:58.074482Z","iopub.status.idle":"2022-08-30T10:40:11.547039Z","shell.execute_reply.started":"2022-08-30T10:39:58.074420Z","shell.execute_reply":"2022-08-30T10:40:11.544647Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\n\nfrom pyspark import SparkConf, SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nfrom pyspark.sql.types import *\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import udf, col","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:11.552630Z","iopub.execute_input":"2022-08-30T10:40:11.553949Z","iopub.status.idle":"2022-08-30T10:40:11.565630Z","shell.execute_reply.started":"2022-08-30T10:40:11.553868Z","shell.execute_reply":"2022-08-30T10:40:11.564195Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"#input / output path\nl3_path = '/kaggle/input/exchangesimul/l3_data_v3.1.csv'\nexpect_l1_path = '/kaggle/input/exchangesimul/expected_l1_data_v3.1.csv'\noutput_path = 'output/l3_data'","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:47:31.062408Z","iopub.execute_input":"2022-08-30T10:47:31.063693Z","iopub.status.idle":"2022-08-30T10:47:31.071486Z","shell.execute_reply.started":"2022-08-30T10:47:31.063597Z","shell.execute_reply":"2022-08-30T10:47:31.070039Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"#spark setup\nspark = SparkSession.builder.master(\"local[*]\").appName(\"L1Generator\").getOrCreate()\nspark","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:11.582792Z","iopub.execute_input":"2022-08-30T10:40:11.583921Z","iopub.status.idle":"2022-08-30T10:40:11.602887Z","shell.execute_reply.started":"2022-08-30T10:40:11.583860Z","shell.execute_reply":"2022-08-30T10:40:11.601379Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"raw_ord_book = spark.read.csv(l3_path, header = True).orderBy('time')\nraw_ord_book.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:11.604450Z","iopub.execute_input":"2022-08-30T10:40:11.605582Z","iopub.status.idle":"2022-08-30T10:40:12.190817Z","shell.execute_reply.started":"2022-08-30T10:40:11.605542Z","shell.execute_reply":"2022-08-30T10:40:12.189358Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"raw_ord_book.createOrReplaceTempView(\"l1_order_book\")\n# raw_ord_book.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:12.192469Z","iopub.execute_input":"2022-08-30T10:40:12.194467Z","iopub.status.idle":"2022-08-30T10:40:12.215633Z","shell.execute_reply.started":"2022-08-30T10:40:12.194401Z","shell.execute_reply":"2022-08-30T10:40:12.214282Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"spark.sql(\"\"\"SELECT * from l1_order_book limit 5\"\"\").show()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:12.217600Z","iopub.execute_input":"2022-08-30T10:40:12.218426Z","iopub.status.idle":"2022-08-30T10:40:12.652256Z","shell.execute_reply.started":"2022-08-30T10:40:12.218377Z","shell.execute_reply":"2022-08-30T10:40:12.650873Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"spark.sql(\"\"\"\n    SELECT \n                *, \n                row_number() OVER(PARTITION BY delete_order_id ORDER BY time ASC) AS del_rank\n            FROM l1_order_book\n\"\"\").orderBy('seq_num').show()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:12.653595Z","iopub.execute_input":"2022-08-30T10:40:12.654610Z","iopub.status.idle":"2022-08-30T10:40:13.787944Z","shell.execute_reply.started":"2022-08-30T10:40:12.654562Z","shell.execute_reply":"2022-08-30T10:40:13.786395Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"\"\"\" \n    This ETL step purpose is to normalize all order type to SELL / BUY order for calculation simplification\n    - For delete order we simply assume as corresponding SELL / BUY order with negative qty as opposed to its counterpart\n    - I didn't understand trade order, and as I explore there is only 4 transaction with this type, I will skip or I can clarify later\n        - Once clarified, can chain 1 more sql e.g trade_order_normalization_sql to take care of trade order.\n\"\"\"\ndelete_order_normalization_sql = \"\"\"\n    SELECT \n        ord_ori.seq_num, \n        ord_ori.time,\n        COALESCE(ord_ori.add_side, ord_ori.delete_side) AS add_side, -- If there are more type of order, use CASE WHEN instead\n        COALESCE(ord_ori.add_price, ord_del.del_add_price) AS add_price,\n        COALESCE(ord_ori.add_qty, CAST(-ord_del.del_add_qty AS INT)) AS add_qty\n    FROM \n        (\n            SELECT \n                *, \n                row_number() OVER(PARTITION BY delete_order_id ORDER BY seq_num ASC) AS del_rank\n            FROM l1_order_book\n        ) AS ord_ori \n        LEFT JOIN (\n            SELECT \n                row_number() OVER(PARTITION BY add_order_id ORDER BY seq_num ASC) AS add_rank,\n                seq_num,\n                add_order_id, \n                add_price AS del_add_price,\n                add_qty AS del_add_qty\n            FROM l1_order_book\n        ) AS ord_del\n        ON ord_ori.delete_order_id = ord_del.add_order_id AND ord_ori.del_rank = ord_del.add_rank \n\"\"\"\n\nnml_ord_book = spark.sql(delete_order_normalization_sql).orderBy('seq_num').cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:13.789917Z","iopub.execute_input":"2022-08-30T10:40:13.790564Z","iopub.status.idle":"2022-08-30T10:40:13.855547Z","shell.execute_reply.started":"2022-08-30T10:40:13.790508Z","shell.execute_reply":"2022-08-30T10:40:13.854189Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"#nml_ord_book.unpersist()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:13.857580Z","iopub.execute_input":"2022-08-30T10:40:13.858076Z","iopub.status.idle":"2022-08-30T10:40:13.865667Z","shell.execute_reply.started":"2022-08-30T10:40:13.858028Z","shell.execute_reply":"2022-08-30T10:40:13.862552Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"# Create Schema for output csv\nfrom pyspark.sql.types import StructType,StructField, StringType\nschema = StructType([\n  StructField('time', StringType(), True),\n  StructField('bid_price', DoubleType(), True),\n  StructField('ask_price', DoubleType(), True),\n  StructField('bid_size', IntegerType(), True),\n  StructField('ask_size', IntegerType(), True),\n  StructField('seq_num', StringType(), True),\n  ])","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:13.867170Z","iopub.execute_input":"2022-08-30T10:40:13.884915Z","iopub.status.idle":"2022-08-30T10:40:13.894840Z","shell.execute_reply.started":"2022-08-30T10:40:13.884840Z","shell.execute_reply":"2022-08-30T10:40:13.893172Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# Main generation logic after order are normalized\nbids = {}\nasks = {}\nans = []\n\ndef generateEntry(seq_num, time):\n    if asks == {} or bids == {}:\n        return\n    minAsk = min(asks.keys())\n    maxBid = max(bids.keys())\n    ans_ent = [time, maxBid, minAsk, bids[maxBid], asks[minAsk], seq_num]\n    if ans:\n        if ans_ent[1:5] == ans[-1][1:5]:\n            return\n    ans.append(ans_ent)\n\ndef updateBook(entry):\n    if(entry['add_side'] == 'BUY'):\n        #print(entry)\n        add_price = float(entry['add_price'])\n        bids[add_price] = bids.get(add_price, 0) + int(entry['add_qty'])\n        if bids[add_price] == 0:\n            del bids[add_price]\n            \n    elif (entry['add_side'] == 'SELL'):\n        #print(entry)\n        add_price = float(entry['add_price'])\n        asks[add_price] = asks.get(add_price, 0) + int(entry['add_qty'])\n        if asks[add_price] == 0:\n            del asks[add_price]\n    generateEntry(entry['seq_num'], entry['time'])\n    \nfor entry in nml_ord_book.rdd.collect():\n    updateBook(entry)\n\nres_df = spark.createDataFrame(ans, schema).orderBy('seq_num').cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:40:13.896205Z","iopub.execute_input":"2022-08-30T10:40:13.896672Z","iopub.status.idle":"2022-08-30T10:40:16.095669Z","shell.execute_reply.started":"2022-08-30T10:40:13.896630Z","shell.execute_reply":"2022-08-30T10:40:16.093654Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"# Write Output\nif os.path.exists(output_path):\n    os.system(\"rm -rf \" + output_path)\n\n# unify back to 1 file, else there will be multiple csv\nres_df.repartition(1).write.csv(output_path)","metadata":{"execution":{"iopub.status.busy":"2022-08-30T10:47:41.041745Z","iopub.execute_input":"2022-08-30T10:47:41.042241Z","iopub.status.idle":"2022-08-30T10:47:42.026391Z","shell.execute_reply.started":"2022-08-30T10:47:41.042204Z","shell.execute_reply":"2022-08-30T10:47:42.024774Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}